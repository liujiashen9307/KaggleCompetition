{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked then averaged with Leaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run Functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.976e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.799e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.799e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.147e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=2.130e-02, previous alpha=2.127e-02, with an active set of 17 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.358e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.489e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.022e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.022e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.022e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=8.001e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.525e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=7.341e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.174e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.780e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=5.767e-03, previous alpha=5.693e-03, with an active set of 32 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.688e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.421e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.421e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.650e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.650e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.516e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.399e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.237e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=2.448e-02, previous alpha=2.216e-02, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.543e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.149e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.602e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.426e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.460e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.414e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.710e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 60 iterations, alpha=2.719e-03, previous alpha=2.707e-03, with an active set of 57 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.301e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.554e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.330e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.330e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.624e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.095e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.095e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.811e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.693e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=2.579e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.474e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.083e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.083e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.517e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.517e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.510e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.404e-03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.404e-03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=1.387e-03, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.362e-03, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.362e-03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.328e-03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.328e-03, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 108 iterations, alpha=1.328e-03, previous alpha=1.292e-03, with an active set of 107 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.931e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.931e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.319e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.048e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.266e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.266e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.963e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.813e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.578e-03, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.531e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.266e-03, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.168e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.929e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.868e-03, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.828e-03, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.718e-03, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 100 iterations, alpha=1.611e-03, previous alpha=1.609e-03, with an active set of 93 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data:\n",
      "0.6578930105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.976e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.799e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.799e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.147e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=2.130e-02, previous alpha=2.127e-02, with an active set of 17 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.358e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.489e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.022e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.022e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.022e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=8.001e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.525e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=7.341e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.174e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.780e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=5.767e-03, previous alpha=5.693e-03, with an active set of 32 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.688e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.421e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.421e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.650e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.650e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.516e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.399e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.237e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=2.448e-02, previous alpha=2.216e-02, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.548e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.176e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.669e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.252e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.905e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.343e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.895e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=2.975e-03, previous alpha=2.774e-03, with an active set of 50 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.591e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.448e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.448e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.828e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.828e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.269e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.269e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.026e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.629e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.431e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.285e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.161e-03, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.161e-03, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.041e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.972e-03, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.837e-03, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.826e-03, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=1.868e-03, previous alpha=1.807e-03, with an active set of 78 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.883e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.301e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.416e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.235e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.207e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.207e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.941e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.829e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.799e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.779e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.502e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.912e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.912e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.872e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.667e-03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.553e-03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.541e-03, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.541e-03, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 100 iterations, alpha=1.536e-03, previous alpha=1.533e-03, with an active set of 99 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data:\n",
      "0.658151843781\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "#usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "\n",
    "'''Train the xgb model then predict the test data'''\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = 0\n",
    "for fold in range(1,3):\n",
    "    np.random.seed(fold)\n",
    "    xgb_params = {\n",
    "        'n_trees': 520, \n",
    "        'eta': 0.0045,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.93,\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'base_score': y_mean, # base prediction = mean(target)\n",
    "        'silent': True,\n",
    "        'seed': fold,\n",
    "    }\n",
    "    # NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "    \n",
    "    dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "    \n",
    "    num_boost_rounds = 1250\n",
    "    # train model\n",
    "    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    '''Train the stacked models then predict the test data'''\n",
    "    \n",
    "    stacked_pipeline = make_pipeline(\n",
    "        StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "        LassoLarsCV()\n",
    "    \n",
    "    )\n",
    "    \n",
    "    stacked_pipeline.fit(finaltrainset, y_train)\n",
    "    results = stacked_pipeline.predict(finaltestset)\n",
    "    \n",
    "    '''R2 Score on the entire Train data when averaging'''\n",
    "    \n",
    "    print('R2 score on train data:')\n",
    "    print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "    \n",
    "    '''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "\n",
    "    sub['y'] += y_pred*0.75 + results*0.25\n",
    "sub['y'] /= 2\n",
    "\n",
    "leaks = {\n",
    "    1:71.34112,\n",
    "    12:109.30903,\n",
    "    23:115.21953,\n",
    "    28:92.00675,\n",
    "    42:87.73572,\n",
    "    43:129.79876,\n",
    "    45:99.55671,\n",
    "    57:116.02167,\n",
    "    3977:132.08556,\n",
    "    88:90.33211,\n",
    "    89:130.55165,\n",
    "    93:105.79792,\n",
    "    94:103.04672,\n",
    "    1001:111.65212,\n",
    "    104:92.37968,\n",
    "    72:110.54742,\n",
    "    78:125.28849,\n",
    "    105:108.5069,\n",
    "    110:83.31692,\n",
    "    1004:91.472,\n",
    "    1008:106.71967,\n",
    "    1009:108.21841,\n",
    "    973:106.76189,\n",
    "    8002:95.84858,\n",
    "    8007:87.44019,\n",
    "    1644:99.14157,\n",
    "    337:101.23135,\n",
    "    253:115.93724,\n",
    "    8416:96.84773,\n",
    "    259:93.33662,\n",
    "    262:75.35182,\n",
    "    1652:89.77625\n",
    "    }\n",
    "sub['y'] = sub.apply(lambda r: leaks[int(r['ID'])] if int(r['ID']) in leaks else r['y'], axis=1)\n",
    "#sub.to_csv('stacked-models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>71.341120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>99.763818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>81.973843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>79.921164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>122.194655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>94.657468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>113.840915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>95.988436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>109.309030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>95.458349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>117.345294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>105.402689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>101.859798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>95.769160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>105.462401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>103.573908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22</td>\n",
       "      <td>117.769115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23</td>\n",
       "      <td>115.219530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>96.503928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>92.006750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29</td>\n",
       "      <td>96.335854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>96.017929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>35</td>\n",
       "      <td>94.576110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>41</td>\n",
       "      <td>96.776331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>42</td>\n",
       "      <td>87.735720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43</td>\n",
       "      <td>129.798760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>45</td>\n",
       "      <td>99.556710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>46</td>\n",
       "      <td>103.899574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>51</td>\n",
       "      <td>94.064526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>53</td>\n",
       "      <td>79.931205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>8361</td>\n",
       "      <td>110.034434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>8363</td>\n",
       "      <td>103.438877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>8364</td>\n",
       "      <td>92.647453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>8365</td>\n",
       "      <td>92.714582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>8366</td>\n",
       "      <td>102.314550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>8370</td>\n",
       "      <td>110.007510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>8372</td>\n",
       "      <td>92.326694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>8376</td>\n",
       "      <td>93.139374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>8377</td>\n",
       "      <td>110.277476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>8379</td>\n",
       "      <td>109.800097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4189</th>\n",
       "      <td>8380</td>\n",
       "      <td>93.167530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>8381</td>\n",
       "      <td>111.348211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>8386</td>\n",
       "      <td>91.695398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>8388</td>\n",
       "      <td>101.633095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>8389</td>\n",
       "      <td>92.256185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>8391</td>\n",
       "      <td>110.712150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>8394</td>\n",
       "      <td>93.257044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>8396</td>\n",
       "      <td>103.207643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>8398</td>\n",
       "      <td>101.799144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>8400</td>\n",
       "      <td>111.434169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>8401</td>\n",
       "      <td>92.428108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>8404</td>\n",
       "      <td>91.638044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>8407</td>\n",
       "      <td>95.799989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>8408</td>\n",
       "      <td>111.804281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>8409</td>\n",
       "      <td>109.636583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>8410</td>\n",
       "      <td>103.272780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>8411</td>\n",
       "      <td>95.394087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>8413</td>\n",
       "      <td>95.927327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>8414</td>\n",
       "      <td>110.377838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>8416</td>\n",
       "      <td>96.847730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID           y\n",
       "0        1   71.341120\n",
       "1        2   99.763818\n",
       "2        3   81.973843\n",
       "3        4   79.921164\n",
       "4        5  122.194655\n",
       "5        8   94.657468\n",
       "6       10  113.840915\n",
       "7       11   95.988436\n",
       "8       12  109.309030\n",
       "9       14   95.458349\n",
       "10      15  117.345294\n",
       "11      16  105.402689\n",
       "12      17  101.859798\n",
       "13      19   95.769160\n",
       "14      20  105.462401\n",
       "15      21  103.573908\n",
       "16      22  117.769115\n",
       "17      23  115.219530\n",
       "18      26   96.503928\n",
       "19      28   92.006750\n",
       "20      29   96.335854\n",
       "21      33   96.017929\n",
       "22      35   94.576110\n",
       "23      41   96.776331\n",
       "24      42   87.735720\n",
       "25      43  129.798760\n",
       "26      45   99.556710\n",
       "27      46  103.899574\n",
       "28      51   94.064526\n",
       "29      53   79.931205\n",
       "...    ...         ...\n",
       "4179  8361  110.034434\n",
       "4180  8363  103.438877\n",
       "4181  8364   92.647453\n",
       "4182  8365   92.714582\n",
       "4183  8366  102.314550\n",
       "4184  8370  110.007510\n",
       "4185  8372   92.326694\n",
       "4186  8376   93.139374\n",
       "4187  8377  110.277476\n",
       "4188  8379  109.800097\n",
       "4189  8380   93.167530\n",
       "4190  8381  111.348211\n",
       "4191  8386   91.695398\n",
       "4192  8388  101.633095\n",
       "4193  8389   92.256185\n",
       "4194  8391  110.712150\n",
       "4195  8394   93.257044\n",
       "4196  8396  103.207643\n",
       "4197  8398  101.799144\n",
       "4198  8400  111.434169\n",
       "4199  8401   92.428108\n",
       "4200  8404   91.638044\n",
       "4201  8407   95.799989\n",
       "4202  8408  111.804281\n",
       "4203  8409  109.636583\n",
       "4204  8410  103.272780\n",
       "4205  8411   95.394087\n",
       "4206  8413   95.927327\n",
       "4207  8414  110.377838\n",
       "4208  8416   96.847730\n",
       "\n",
       "[4209 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A NNet with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Fold: 0, R2 Score: -4.7507\n",
      "Step: 0, Fold: 1, R2 Score: -3.81577\n",
      "Step: 0, Fold: 2, R2 Score: -4.30285\n",
      "Step: 0, Fold: 3, R2 Score: -4.46236\n",
      "Step: 0, Fold: 4, R2 Score: -4.16163\n",
      "Step: 0, Fold: 5, R2 Score: -4.14147\n",
      "Step: 0, Fold: 6, R2 Score: -4.05315\n",
      "Step: 0, Fold: 7, R2 Score: -4.95255\n",
      "Step: 0, Fold: 8, R2 Score: -5.01364\n",
      "Step: 0, Fold: 9, R2 Score: -4.46376\n",
      "Step: 10, Fold: 0, R2 Score: -0.262886\n",
      "Step: 10, Fold: 1, R2 Score: -0.184338\n",
      "Step: 10, Fold: 2, R2 Score: -0.157193\n",
      "Step: 10, Fold: 3, R2 Score: -0.18155\n",
      "Step: 10, Fold: 4, R2 Score: -0.191545\n",
      "Step: 10, Fold: 5, R2 Score: -0.0717292\n",
      "Step: 10, Fold: 6, R2 Score: -0.0736445\n",
      "Step: 10, Fold: 7, R2 Score: -0.144103\n",
      "Step: 10, Fold: 8, R2 Score: -0.134922\n",
      "Step: 10, Fold: 9, R2 Score: -0.0880809\n",
      "Step: 20, Fold: 0, R2 Score: -0.101706\n",
      "Step: 20, Fold: 1, R2 Score: -0.0589188\n",
      "Step: 20, Fold: 2, R2 Score: -0.0518843\n",
      "Step: 20, Fold: 3, R2 Score: -0.101785\n",
      "Step: 20, Fold: 4, R2 Score: -0.113034\n",
      "Step: 20, Fold: 5, R2 Score: -0.0577997\n",
      "Step: 20, Fold: 6, R2 Score: -0.0672827\n",
      "Step: 20, Fold: 7, R2 Score: 0.0651049\n",
      "Step: 20, Fold: 8, R2 Score: 0.00845746\n",
      "Step: 20, Fold: 9, R2 Score: -0.052065\n",
      "Step: 30, Fold: 0, R2 Score: 0.164961\n",
      "Step: 30, Fold: 1, R2 Score: 0.077248\n",
      "Step: 30, Fold: 2, R2 Score: 0.172016\n",
      "Step: 30, Fold: 3, R2 Score: 0.194614\n",
      "Step: 30, Fold: 4, R2 Score: 0.188259\n",
      "Step: 30, Fold: 5, R2 Score: 0.119636\n",
      "Step: 30, Fold: 6, R2 Score: 0.088153\n",
      "Step: 30, Fold: 7, R2 Score: 0.119509\n",
      "Step: 30, Fold: 8, R2 Score: 0.092513\n",
      "Step: 30, Fold: 9, R2 Score: 0.192645\n",
      "Step: 40, Fold: 0, R2 Score: 0.412681\n",
      "Step: 40, Fold: 1, R2 Score: 0.335499\n",
      "Step: 40, Fold: 2, R2 Score: 0.397734\n",
      "Step: 40, Fold: 3, R2 Score: 0.396427\n",
      "Step: 40, Fold: 4, R2 Score: 0.381904\n",
      "Step: 40, Fold: 5, R2 Score: 0.357774\n",
      "Step: 40, Fold: 6, R2 Score: 0.418111\n",
      "Step: 40, Fold: 7, R2 Score: 0.373644\n",
      "Step: 40, Fold: 8, R2 Score: 0.39843\n",
      "Step: 40, Fold: 9, R2 Score: 0.388202\n",
      "Step: 50, Fold: 0, R2 Score: 0.444858\n",
      "Step: 50, Fold: 1, R2 Score: 0.437759\n",
      "Step: 50, Fold: 2, R2 Score: 0.546601\n",
      "Step: 50, Fold: 3, R2 Score: 0.521909\n",
      "Step: 50, Fold: 4, R2 Score: 0.638917\n",
      "Step: 50, Fold: 5, R2 Score: 0.552649\n",
      "Step: 50, Fold: 6, R2 Score: 0.536506\n",
      "Step: 50, Fold: 7, R2 Score: 0.595044\n",
      "Step: 50, Fold: 8, R2 Score: 0.540339\n",
      "Step: 50, Fold: 9, R2 Score: 0.480463\n",
      "Step: 60, Fold: 0, R2 Score: 0.586204\n",
      "Step: 60, Fold: 1, R2 Score: 0.552235\n",
      "Step: 60, Fold: 2, R2 Score: 0.562922\n",
      "Step: 60, Fold: 3, R2 Score: 0.586997\n",
      "Step: 60, Fold: 4, R2 Score: 0.578797\n",
      "Step: 60, Fold: 5, R2 Score: 0.588961\n",
      "Step: 60, Fold: 6, R2 Score: 0.559356\n",
      "Step: 60, Fold: 7, R2 Score: 0.542465\n",
      "Step: 60, Fold: 8, R2 Score: 0.526912\n",
      "Step: 60, Fold: 9, R2 Score: 0.595756\n",
      "Step: 70, Fold: 0, R2 Score: 0.592872\n",
      "Step: 70, Fold: 1, R2 Score: 0.604053\n",
      "Step: 70, Fold: 2, R2 Score: 0.547054\n",
      "Step: 70, Fold: 3, R2 Score: 0.62223\n",
      "Step: 70, Fold: 4, R2 Score: 0.522411\n",
      "Step: 70, Fold: 5, R2 Score: 0.531781\n",
      "Step: 70, Fold: 6, R2 Score: 0.626744\n",
      "Step: 70, Fold: 7, R2 Score: 0.544632\n",
      "Step: 70, Fold: 8, R2 Score: 0.594013\n",
      "Step: 70, Fold: 9, R2 Score: 0.548575\n",
      "Step: 80, Fold: 0, R2 Score: 0.574475\n",
      "Step: 80, Fold: 1, R2 Score: 0.540061\n",
      "Step: 80, Fold: 2, R2 Score: 0.531119\n",
      "Step: 80, Fold: 3, R2 Score: 0.560557\n",
      "Step: 80, Fold: 4, R2 Score: 0.565284\n",
      "Step: 80, Fold: 5, R2 Score: 0.627786\n",
      "Step: 80, Fold: 6, R2 Score: 0.551462\n",
      "Step: 80, Fold: 7, R2 Score: 0.633962\n",
      "Step: 80, Fold: 8, R2 Score: 0.561217\n",
      "Step: 80, Fold: 9, R2 Score: 0.611742\n",
      "Step: 90, Fold: 0, R2 Score: 0.614182\n",
      "Step: 90, Fold: 1, R2 Score: 0.65915\n",
      "Step: 90, Fold: 2, R2 Score: 0.601698\n",
      "Step: 90, Fold: 3, R2 Score: 0.569396\n",
      "Step: 90, Fold: 4, R2 Score: 0.565732\n",
      "Step: 90, Fold: 5, R2 Score: 0.54904\n",
      "Step: 90, Fold: 6, R2 Score: 0.526454\n",
      "Step: 90, Fold: 7, R2 Score: 0.5204\n",
      "Step: 90, Fold: 8, R2 Score: 0.641587\n",
      "Step: 90, Fold: 9, R2 Score: 0.543794\n",
      "Step: 100, Fold: 0, R2 Score: 0.609108\n",
      "Step: 100, Fold: 1, R2 Score: 0.539422\n",
      "Step: 100, Fold: 2, R2 Score: 0.519744\n",
      "Step: 100, Fold: 3, R2 Score: 0.512425\n",
      "Step: 100, Fold: 4, R2 Score: 0.612796\n",
      "Step: 100, Fold: 5, R2 Score: 0.632404\n",
      "Step: 100, Fold: 6, R2 Score: 0.557145\n",
      "Step: 100, Fold: 7, R2 Score: 0.562573\n",
      "Step: 100, Fold: 8, R2 Score: 0.598258\n",
      "Step: 100, Fold: 9, R2 Score: 0.631953\n",
      "Step: 110, Fold: 0, R2 Score: 0.614513\n",
      "Step: 110, Fold: 1, R2 Score: 0.599315\n",
      "Step: 110, Fold: 2, R2 Score: 0.590653\n",
      "Step: 110, Fold: 3, R2 Score: 0.518563\n",
      "Step: 110, Fold: 4, R2 Score: 0.471983\n",
      "Step: 110, Fold: 5, R2 Score: 0.567799\n",
      "Step: 110, Fold: 6, R2 Score: 0.65247\n",
      "Step: 110, Fold: 7, R2 Score: 0.5864\n",
      "Step: 110, Fold: 8, R2 Score: 0.636007\n",
      "Step: 110, Fold: 9, R2 Score: 0.553024\n",
      "Step: 120, Fold: 0, R2 Score: 0.553101\n",
      "Step: 120, Fold: 1, R2 Score: 0.622437\n",
      "Step: 120, Fold: 2, R2 Score: 0.635544\n",
      "Step: 120, Fold: 3, R2 Score: 0.620078\n",
      "Step: 120, Fold: 4, R2 Score: 0.474518\n",
      "Step: 120, Fold: 5, R2 Score: 0.62398\n",
      "Step: 120, Fold: 6, R2 Score: 0.542124\n",
      "Step: 120, Fold: 7, R2 Score: 0.598518\n",
      "Step: 120, Fold: 8, R2 Score: 0.590391\n",
      "Step: 120, Fold: 9, R2 Score: 0.542154\n",
      "Step: 130, Fold: 0, R2 Score: 0.60554\n",
      "Step: 130, Fold: 1, R2 Score: 0.550019\n",
      "Step: 130, Fold: 2, R2 Score: 0.688494\n",
      "Step: 130, Fold: 3, R2 Score: 0.665406\n",
      "Step: 130, Fold: 4, R2 Score: 0.580371\n",
      "Step: 130, Fold: 5, R2 Score: 0.54316\n",
      "Step: 130, Fold: 6, R2 Score: 0.561139\n",
      "Step: 130, Fold: 7, R2 Score: 0.574106\n",
      "Step: 130, Fold: 8, R2 Score: 0.522065\n",
      "Step: 130, Fold: 9, R2 Score: 0.531743\n",
      "Step: 140, Fold: 0, R2 Score: 0.586427\n",
      "Step: 140, Fold: 1, R2 Score: 0.601045\n",
      "Step: 140, Fold: 2, R2 Score: 0.508485\n",
      "Step: 140, Fold: 3, R2 Score: 0.52026\n",
      "Step: 140, Fold: 4, R2 Score: 0.509973\n",
      "Step: 140, Fold: 5, R2 Score: 0.568728\n",
      "Step: 140, Fold: 6, R2 Score: 0.611099\n",
      "Step: 140, Fold: 7, R2 Score: 0.621081\n",
      "Step: 140, Fold: 8, R2 Score: 0.69132\n",
      "Step: 140, Fold: 9, R2 Score: 0.629021\n",
      "Step: 150, Fold: 0, R2 Score: 0.558311\n",
      "Step: 150, Fold: 1, R2 Score: 0.598699\n",
      "Step: 150, Fold: 2, R2 Score: 0.572776\n",
      "Step: 150, Fold: 3, R2 Score: 0.628244\n",
      "Step: 150, Fold: 4, R2 Score: 0.581143\n",
      "Step: 150, Fold: 5, R2 Score: 0.602922\n",
      "Step: 150, Fold: 6, R2 Score: 0.517319\n",
      "Step: 150, Fold: 7, R2 Score: 0.535712\n",
      "Step: 150, Fold: 8, R2 Score: 0.599389\n",
      "Step: 150, Fold: 9, R2 Score: 0.632125\n",
      "Step: 160, Fold: 0, R2 Score: 0.580264\n",
      "Step: 160, Fold: 1, R2 Score: 0.604262\n",
      "Step: 160, Fold: 2, R2 Score: 0.592949\n",
      "Step: 160, Fold: 3, R2 Score: 0.619276\n",
      "Step: 160, Fold: 4, R2 Score: 0.578849\n",
      "Step: 160, Fold: 5, R2 Score: 0.564581\n",
      "Step: 160, Fold: 6, R2 Score: 0.595916\n",
      "Step: 160, Fold: 7, R2 Score: 0.611463\n",
      "Step: 160, Fold: 8, R2 Score: 0.529599\n",
      "Step: 160, Fold: 9, R2 Score: 0.560492\n",
      "Step: 170, Fold: 0, R2 Score: 0.648868\n",
      "Step: 170, Fold: 1, R2 Score: 0.584884\n",
      "Step: 170, Fold: 2, R2 Score: 0.554142\n",
      "Step: 170, Fold: 3, R2 Score: 0.449732\n",
      "Step: 170, Fold: 4, R2 Score: 0.628198\n",
      "Step: 170, Fold: 5, R2 Score: 0.523365\n",
      "Step: 170, Fold: 6, R2 Score: 0.608467\n",
      "Step: 170, Fold: 7, R2 Score: 0.662073\n",
      "Step: 170, Fold: 8, R2 Score: 0.607742\n",
      "Step: 170, Fold: 9, R2 Score: 0.605705\n",
      "Step: 180, Fold: 0, R2 Score: 0.613868\n",
      "Step: 180, Fold: 1, R2 Score: 0.630077\n",
      "Step: 180, Fold: 2, R2 Score: 0.557831\n",
      "Step: 180, Fold: 3, R2 Score: 0.609433\n",
      "Step: 180, Fold: 4, R2 Score: 0.609673\n",
      "Step: 180, Fold: 5, R2 Score: 0.562396\n",
      "Step: 180, Fold: 6, R2 Score: 0.594888\n",
      "Step: 180, Fold: 7, R2 Score: 0.594629\n",
      "Step: 180, Fold: 8, R2 Score: 0.522631\n",
      "Step: 180, Fold: 9, R2 Score: 0.589819\n",
      "Step: 190, Fold: 0, R2 Score: 0.591441\n",
      "Step: 190, Fold: 1, R2 Score: 0.631091\n",
      "Step: 190, Fold: 2, R2 Score: 0.615847\n",
      "Step: 190, Fold: 3, R2 Score: 0.550741\n",
      "Step: 190, Fold: 4, R2 Score: 0.607661\n",
      "Step: 190, Fold: 5, R2 Score: 0.627089\n",
      "Step: 190, Fold: 6, R2 Score: 0.502359\n",
      "Step: 190, Fold: 7, R2 Score: 0.646109\n",
      "Step: 190, Fold: 8, R2 Score: 0.486983\n",
      "Step: 190, Fold: 9, R2 Score: 0.678361\n",
      "Step: 200, Fold: 0, R2 Score: 0.619917\n",
      "Step: 200, Fold: 1, R2 Score: 0.643729\n",
      "Step: 200, Fold: 2, R2 Score: 0.635207\n",
      "Step: 200, Fold: 3, R2 Score: 0.539544\n",
      "Step: 200, Fold: 4, R2 Score: 0.594867\n",
      "Step: 200, Fold: 5, R2 Score: 0.555941\n",
      "Step: 200, Fold: 6, R2 Score: 0.573656\n",
      "Step: 200, Fold: 7, R2 Score: 0.603998\n",
      "Step: 200, Fold: 8, R2 Score: 0.595758\n",
      "Step: 200, Fold: 9, R2 Score: 0.551196\n",
      "Step: 210, Fold: 0, R2 Score: 0.518726\n",
      "Step: 210, Fold: 1, R2 Score: 0.59922\n",
      "Step: 210, Fold: 2, R2 Score: 0.515879\n",
      "Step: 210, Fold: 3, R2 Score: 0.6505\n",
      "Step: 210, Fold: 4, R2 Score: 0.613099\n",
      "Step: 210, Fold: 5, R2 Score: 0.606972\n",
      "Step: 210, Fold: 6, R2 Score: 0.586963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 210, Fold: 7, R2 Score: 0.599904\n",
      "Step: 210, Fold: 8, R2 Score: 0.621885\n",
      "Step: 210, Fold: 9, R2 Score: 0.602897\n",
      "Step: 220, Fold: 0, R2 Score: 0.545358\n",
      "Step: 220, Fold: 1, R2 Score: 0.640202\n",
      "Step: 220, Fold: 2, R2 Score: 0.597606\n",
      "Step: 220, Fold: 3, R2 Score: 0.537857\n",
      "Step: 220, Fold: 4, R2 Score: 0.56837\n",
      "Step: 220, Fold: 5, R2 Score: 0.598081\n",
      "Step: 220, Fold: 6, R2 Score: 0.570602\n",
      "Step: 220, Fold: 7, R2 Score: 0.662485\n",
      "Step: 220, Fold: 8, R2 Score: 0.627427\n",
      "Step: 220, Fold: 9, R2 Score: 0.598282\n",
      "Step: 230, Fold: 0, R2 Score: 0.61397\n",
      "Step: 230, Fold: 1, R2 Score: 0.628249\n",
      "Step: 230, Fold: 2, R2 Score: 0.619514\n",
      "Step: 230, Fold: 3, R2 Score: 0.564641\n",
      "Step: 230, Fold: 4, R2 Score: 0.600018\n",
      "Step: 230, Fold: 5, R2 Score: 0.551985\n",
      "Step: 230, Fold: 6, R2 Score: 0.624007\n",
      "Step: 230, Fold: 7, R2 Score: 0.587823\n",
      "Step: 230, Fold: 8, R2 Score: 0.577407\n",
      "Step: 230, Fold: 9, R2 Score: 0.590295\n",
      "Step: 240, Fold: 0, R2 Score: 0.620987\n",
      "Step: 240, Fold: 1, R2 Score: 0.584113\n",
      "Step: 240, Fold: 2, R2 Score: 0.625709\n",
      "Step: 240, Fold: 3, R2 Score: 0.609948\n",
      "Step: 240, Fold: 4, R2 Score: 0.565988\n",
      "Step: 240, Fold: 5, R2 Score: 0.648119\n",
      "Step: 240, Fold: 6, R2 Score: 0.540618\n",
      "Step: 240, Fold: 7, R2 Score: 0.580604\n",
      "Step: 240, Fold: 8, R2 Score: 0.54953\n",
      "Step: 240, Fold: 9, R2 Score: 0.642548\n",
      "Step: 250, Fold: 0, R2 Score: 0.590831\n",
      "Step: 250, Fold: 1, R2 Score: 0.630947\n",
      "Step: 250, Fold: 2, R2 Score: 0.519223\n",
      "Step: 250, Fold: 3, R2 Score: 0.644013\n",
      "Step: 250, Fold: 4, R2 Score: 0.575789\n",
      "Step: 250, Fold: 5, R2 Score: 0.648556\n",
      "Step: 250, Fold: 6, R2 Score: 0.609514\n",
      "Step: 250, Fold: 7, R2 Score: 0.575345\n",
      "Step: 250, Fold: 8, R2 Score: 0.569071\n",
      "Step: 250, Fold: 9, R2 Score: 0.602031\n",
      "Step: 260, Fold: 0, R2 Score: 0.577275\n",
      "Step: 260, Fold: 1, R2 Score: 0.599777\n",
      "Step: 260, Fold: 2, R2 Score: 0.577927\n",
      "Step: 260, Fold: 3, R2 Score: 0.564203\n",
      "Step: 260, Fold: 4, R2 Score: 0.554043\n",
      "Step: 260, Fold: 5, R2 Score: 0.584529\n",
      "Step: 260, Fold: 6, R2 Score: 0.689233\n",
      "Step: 260, Fold: 7, R2 Score: 0.667454\n",
      "Step: 260, Fold: 8, R2 Score: 0.559856\n",
      "Step: 260, Fold: 9, R2 Score: 0.620059\n",
      "Step: 270, Fold: 0, R2 Score: 0.543823\n",
      "Step: 270, Fold: 1, R2 Score: 0.587799\n",
      "Step: 270, Fold: 2, R2 Score: 0.519813\n",
      "Step: 270, Fold: 3, R2 Score: 0.587685\n",
      "Step: 270, Fold: 4, R2 Score: 0.599216\n",
      "Step: 270, Fold: 5, R2 Score: 0.629647\n",
      "Step: 270, Fold: 6, R2 Score: 0.654584\n",
      "Step: 270, Fold: 7, R2 Score: 0.597695\n",
      "Step: 270, Fold: 8, R2 Score: 0.629899\n",
      "Step: 270, Fold: 9, R2 Score: 0.617618\n",
      "Step: 280, Fold: 0, R2 Score: 0.60971\n",
      "Step: 280, Fold: 1, R2 Score: 0.550152\n",
      "Step: 280, Fold: 2, R2 Score: 0.566787\n",
      "Step: 280, Fold: 3, R2 Score: 0.574234\n",
      "Step: 280, Fold: 4, R2 Score: 0.646985\n",
      "Step: 280, Fold: 5, R2 Score: 0.582953\n",
      "Step: 280, Fold: 6, R2 Score: 0.591359\n",
      "Step: 280, Fold: 7, R2 Score: 0.592441\n",
      "Step: 280, Fold: 8, R2 Score: 0.668491\n",
      "Step: 280, Fold: 9, R2 Score: 0.589026\n",
      "Step: 290, Fold: 0, R2 Score: 0.554801\n",
      "Step: 290, Fold: 1, R2 Score: 0.57619\n",
      "Step: 290, Fold: 2, R2 Score: 0.657012\n",
      "Step: 290, Fold: 3, R2 Score: 0.630793\n",
      "Step: 290, Fold: 4, R2 Score: 0.641108\n",
      "Step: 290, Fold: 5, R2 Score: 0.672528\n",
      "Step: 290, Fold: 6, R2 Score: 0.527873\n",
      "Step: 290, Fold: 7, R2 Score: 0.585279\n",
      "Step: 290, Fold: 8, R2 Score: 0.59522\n",
      "Step: 290, Fold: 9, R2 Score: 0.548521\n",
      "Step: 300, Fold: 0, R2 Score: 0.551482\n",
      "Step: 300, Fold: 1, R2 Score: 0.578783\n",
      "Step: 300, Fold: 2, R2 Score: 0.585486\n",
      "Step: 300, Fold: 3, R2 Score: 0.604632\n",
      "Step: 300, Fold: 4, R2 Score: 0.593232\n",
      "Step: 300, Fold: 5, R2 Score: 0.605643\n",
      "Step: 300, Fold: 6, R2 Score: 0.616436\n",
      "Step: 300, Fold: 7, R2 Score: 0.624644\n",
      "Step: 300, Fold: 8, R2 Score: 0.66228\n",
      "Step: 300, Fold: 9, R2 Score: 0.566615\n",
      "Step: 310, Fold: 0, R2 Score: 0.597739\n",
      "Step: 310, Fold: 1, R2 Score: 0.562615\n",
      "Step: 310, Fold: 2, R2 Score: 0.590186\n",
      "Step: 310, Fold: 3, R2 Score: 0.59464\n",
      "Step: 310, Fold: 4, R2 Score: 0.636377\n",
      "Step: 310, Fold: 5, R2 Score: 0.649419\n",
      "Step: 310, Fold: 6, R2 Score: 0.570161\n",
      "Step: 310, Fold: 7, R2 Score: 0.601471\n",
      "Step: 310, Fold: 8, R2 Score: 0.648763\n",
      "Step: 310, Fold: 9, R2 Score: 0.543906\n",
      "Step: 320, Fold: 0, R2 Score: 0.498574\n",
      "Step: 320, Fold: 1, R2 Score: 0.651547\n",
      "Step: 320, Fold: 2, R2 Score: 0.618516\n",
      "Step: 320, Fold: 3, R2 Score: 0.608625\n",
      "Step: 320, Fold: 4, R2 Score: 0.596031\n",
      "Step: 320, Fold: 5, R2 Score: 0.578732\n",
      "Step: 320, Fold: 6, R2 Score: 0.598639\n",
      "Step: 320, Fold: 7, R2 Score: 0.618014\n",
      "Step: 320, Fold: 8, R2 Score: 0.600447\n",
      "Step: 320, Fold: 9, R2 Score: 0.617811\n",
      "Step: 330, Fold: 0, R2 Score: 0.543966\n",
      "Step: 330, Fold: 1, R2 Score: 0.606939\n",
      "Step: 330, Fold: 2, R2 Score: 0.662785\n",
      "Step: 330, Fold: 3, R2 Score: 0.627702\n",
      "Step: 330, Fold: 4, R2 Score: 0.570026\n",
      "Step: 330, Fold: 5, R2 Score: 0.660444\n",
      "Step: 330, Fold: 6, R2 Score: 0.577991\n",
      "Step: 330, Fold: 7, R2 Score: 0.543959\n",
      "Step: 330, Fold: 8, R2 Score: 0.593074\n",
      "Step: 330, Fold: 9, R2 Score: 0.625807\n",
      "Step: 340, Fold: 0, R2 Score: 0.622492\n",
      "Step: 340, Fold: 1, R2 Score: 0.567538\n",
      "Step: 340, Fold: 2, R2 Score: 0.621292\n",
      "Step: 340, Fold: 3, R2 Score: 0.559336\n",
      "Step: 340, Fold: 4, R2 Score: 0.657932\n",
      "Step: 340, Fold: 5, R2 Score: 0.609644\n",
      "Step: 340, Fold: 6, R2 Score: 0.620605\n",
      "Step: 340, Fold: 7, R2 Score: 0.593249\n",
      "Step: 340, Fold: 8, R2 Score: 0.556942\n",
      "Step: 340, Fold: 9, R2 Score: 0.583883\n",
      "Step: 350, Fold: 0, R2 Score: 0.583009\n",
      "Step: 350, Fold: 1, R2 Score: 0.575905\n",
      "Step: 350, Fold: 2, R2 Score: 0.66736\n",
      "Step: 350, Fold: 3, R2 Score: 0.49643\n",
      "Step: 350, Fold: 4, R2 Score: 0.612201\n",
      "Step: 350, Fold: 5, R2 Score: 0.607237\n",
      "Step: 350, Fold: 6, R2 Score: 0.599393\n",
      "Step: 350, Fold: 7, R2 Score: 0.586178\n",
      "Step: 350, Fold: 8, R2 Score: 0.671111\n",
      "Step: 350, Fold: 9, R2 Score: 0.607691\n",
      "Step: 360, Fold: 0, R2 Score: 0.606563\n",
      "Step: 360, Fold: 1, R2 Score: 0.596371\n",
      "Step: 360, Fold: 2, R2 Score: 0.552245\n",
      "Step: 360, Fold: 3, R2 Score: 0.640578\n",
      "Step: 360, Fold: 4, R2 Score: 0.609205\n",
      "Step: 360, Fold: 5, R2 Score: 0.585208\n",
      "Step: 360, Fold: 6, R2 Score: 0.525054\n",
      "Step: 360, Fold: 7, R2 Score: 0.652456\n",
      "Step: 360, Fold: 8, R2 Score: 0.626235\n",
      "Step: 360, Fold: 9, R2 Score: 0.611415\n",
      "Step: 370, Fold: 0, R2 Score: 0.57522\n",
      "Step: 370, Fold: 1, R2 Score: 0.673781\n",
      "Step: 370, Fold: 2, R2 Score: 0.586432\n",
      "Step: 370, Fold: 3, R2 Score: 0.614232\n",
      "Step: 370, Fold: 4, R2 Score: 0.591421\n",
      "Step: 370, Fold: 5, R2 Score: 0.673413\n",
      "Step: 370, Fold: 6, R2 Score: 0.579965\n",
      "Step: 370, Fold: 7, R2 Score: 0.574044\n",
      "Step: 370, Fold: 8, R2 Score: 0.57427\n",
      "Step: 370, Fold: 9, R2 Score: 0.569892\n",
      "Step: 380, Fold: 0, R2 Score: 0.605107\n",
      "Step: 380, Fold: 1, R2 Score: 0.542726\n",
      "Step: 380, Fold: 2, R2 Score: 0.620662\n",
      "Step: 380, Fold: 3, R2 Score: 0.563754\n",
      "Step: 380, Fold: 4, R2 Score: 0.516562\n",
      "Step: 380, Fold: 5, R2 Score: 0.637493\n",
      "Step: 380, Fold: 6, R2 Score: 0.63166\n",
      "Step: 380, Fold: 7, R2 Score: 0.602975\n",
      "Step: 380, Fold: 8, R2 Score: 0.686261\n",
      "Step: 380, Fold: 9, R2 Score: 0.599595\n",
      "Step: 390, Fold: 0, R2 Score: 0.592853\n",
      "Step: 390, Fold: 1, R2 Score: 0.617846\n",
      "Step: 390, Fold: 2, R2 Score: 0.610562\n",
      "Step: 390, Fold: 3, R2 Score: 0.635059\n",
      "Step: 390, Fold: 4, R2 Score: 0.580418\n",
      "Step: 390, Fold: 5, R2 Score: 0.606878\n",
      "Step: 390, Fold: 6, R2 Score: 0.498581\n",
      "Step: 390, Fold: 7, R2 Score: 0.639582\n",
      "Step: 390, Fold: 8, R2 Score: 0.603731\n",
      "Step: 390, Fold: 9, R2 Score: 0.619588\n",
      "Step: 400, Fold: 0, R2 Score: 0.520635\n",
      "Step: 400, Fold: 1, R2 Score: 0.641907\n",
      "Step: 400, Fold: 2, R2 Score: 0.581467\n",
      "Step: 400, Fold: 3, R2 Score: 0.613005\n",
      "Step: 400, Fold: 4, R2 Score: 0.572328\n",
      "Step: 400, Fold: 5, R2 Score: 0.584063\n",
      "Step: 400, Fold: 6, R2 Score: 0.600101\n",
      "Step: 400, Fold: 7, R2 Score: 0.672718\n",
      "Step: 400, Fold: 8, R2 Score: 0.647884\n",
      "Step: 400, Fold: 9, R2 Score: 0.5778\n",
      "Step: 410, Fold: 0, R2 Score: 0.629893\n",
      "Step: 410, Fold: 1, R2 Score: 0.648394\n",
      "Step: 410, Fold: 2, R2 Score: 0.564201\n",
      "Step: 410, Fold: 3, R2 Score: 0.608221\n",
      "Step: 410, Fold: 4, R2 Score: 0.596833\n",
      "Step: 410, Fold: 5, R2 Score: 0.603167\n",
      "Step: 410, Fold: 6, R2 Score: 0.561289\n",
      "Step: 410, Fold: 7, R2 Score: 0.620911\n",
      "Step: 410, Fold: 8, R2 Score: 0.538381\n",
      "Step: 410, Fold: 9, R2 Score: 0.622484\n",
      "Step: 420, Fold: 0, R2 Score: 0.529569\n",
      "Step: 420, Fold: 1, R2 Score: 0.556459\n",
      "Step: 420, Fold: 2, R2 Score: 0.635596\n",
      "Step: 420, Fold: 3, R2 Score: 0.625593\n",
      "Step: 420, Fold: 4, R2 Score: 0.689417\n",
      "Step: 420, Fold: 5, R2 Score: 0.548575\n",
      "Step: 420, Fold: 6, R2 Score: 0.587488\n",
      "Step: 420, Fold: 7, R2 Score: 0.617487\n",
      "Step: 420, Fold: 8, R2 Score: 0.561601\n",
      "Step: 420, Fold: 9, R2 Score: 0.647488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 430, Fold: 0, R2 Score: 0.617738\n",
      "Step: 430, Fold: 1, R2 Score: 0.60328\n",
      "Step: 430, Fold: 2, R2 Score: 0.530473\n",
      "Step: 430, Fold: 3, R2 Score: 0.565296\n",
      "Step: 430, Fold: 4, R2 Score: 0.621615\n",
      "Step: 430, Fold: 5, R2 Score: 0.52546\n",
      "Step: 430, Fold: 6, R2 Score: 0.572421\n",
      "Step: 430, Fold: 7, R2 Score: 0.656016\n",
      "Step: 430, Fold: 8, R2 Score: 0.639338\n",
      "Step: 430, Fold: 9, R2 Score: 0.676954\n",
      "Step: 440, Fold: 0, R2 Score: 0.609136\n",
      "Step: 440, Fold: 1, R2 Score: 0.6944\n",
      "Step: 440, Fold: 2, R2 Score: 0.60982\n",
      "Step: 440, Fold: 3, R2 Score: 0.591778\n",
      "Step: 440, Fold: 4, R2 Score: 0.53584\n",
      "Step: 440, Fold: 5, R2 Score: 0.595109\n",
      "Step: 440, Fold: 6, R2 Score: 0.603682\n",
      "Step: 440, Fold: 7, R2 Score: 0.579383\n",
      "Step: 440, Fold: 8, R2 Score: 0.628436\n",
      "Step: 440, Fold: 9, R2 Score: 0.584086\n",
      "Step: 450, Fold: 0, R2 Score: 0.631948\n",
      "Step: 450, Fold: 1, R2 Score: 0.639744\n",
      "Step: 450, Fold: 2, R2 Score: 0.479207\n",
      "Step: 450, Fold: 3, R2 Score: 0.592714\n",
      "Step: 450, Fold: 4, R2 Score: 0.620391\n",
      "Step: 450, Fold: 5, R2 Score: 0.590581\n",
      "Step: 450, Fold: 6, R2 Score: 0.66038\n",
      "Step: 450, Fold: 7, R2 Score: 0.567343\n",
      "Step: 450, Fold: 8, R2 Score: 0.598463\n",
      "Step: 450, Fold: 9, R2 Score: 0.639648\n",
      "Step: 460, Fold: 0, R2 Score: 0.604416\n",
      "Step: 460, Fold: 1, R2 Score: 0.66974\n",
      "Step: 460, Fold: 2, R2 Score: 0.664501\n",
      "Step: 460, Fold: 3, R2 Score: 0.611088\n",
      "Step: 460, Fold: 4, R2 Score: 0.554709\n",
      "Step: 460, Fold: 5, R2 Score: 0.588957\n",
      "Step: 460, Fold: 6, R2 Score: 0.649874\n",
      "Step: 460, Fold: 7, R2 Score: 0.556744\n",
      "Step: 460, Fold: 8, R2 Score: 0.59402\n",
      "Step: 460, Fold: 9, R2 Score: 0.544767\n",
      "Step: 470, Fold: 0, R2 Score: 0.558468\n",
      "Step: 470, Fold: 1, R2 Score: 0.552422\n",
      "Step: 470, Fold: 2, R2 Score: 0.566552\n",
      "Step: 470, Fold: 3, R2 Score: 0.627662\n",
      "Step: 470, Fold: 4, R2 Score: 0.657311\n",
      "Step: 470, Fold: 5, R2 Score: 0.596512\n",
      "Step: 470, Fold: 6, R2 Score: 0.676766\n",
      "Step: 470, Fold: 7, R2 Score: 0.553433\n",
      "Step: 470, Fold: 8, R2 Score: 0.580286\n",
      "Step: 470, Fold: 9, R2 Score: 0.653497\n",
      "Step: 480, Fold: 0, R2 Score: 0.656549\n",
      "Step: 480, Fold: 1, R2 Score: 0.521408\n",
      "Step: 480, Fold: 2, R2 Score: 0.605286\n",
      "Step: 480, Fold: 3, R2 Score: 0.568624\n",
      "Step: 480, Fold: 4, R2 Score: 0.589315\n",
      "Step: 480, Fold: 5, R2 Score: 0.643676\n",
      "Step: 480, Fold: 6, R2 Score: 0.593271\n",
      "Step: 480, Fold: 7, R2 Score: 0.628059\n",
      "Step: 480, Fold: 8, R2 Score: 0.576384\n",
      "Step: 480, Fold: 9, R2 Score: 0.629623\n",
      "Step: 490, Fold: 0, R2 Score: 0.666936\n",
      "Step: 490, Fold: 1, R2 Score: 0.635536\n",
      "Step: 490, Fold: 2, R2 Score: 0.597037\n",
      "Step: 490, Fold: 3, R2 Score: 0.589186\n",
      "Step: 490, Fold: 4, R2 Score: 0.604795\n",
      "Step: 490, Fold: 5, R2 Score: 0.564922\n",
      "Step: 490, Fold: 6, R2 Score: 0.639608\n",
      "Step: 490, Fold: 7, R2 Score: 0.587437\n",
      "Step: 490, Fold: 8, R2 Score: 0.655708\n",
      "Step: 490, Fold: 9, R2 Score: 0.500328\n",
      "Step: 0, Fold: 0, R2 Score: 0.715468\n",
      "Step: 0, Fold: 1, R2 Score: 0.611782\n",
      "Step: 0, Fold: 2, R2 Score: 0.460343\n",
      "Step: 0, Fold: 3, R2 Score: 0.590431\n",
      "Step: 0, Fold: 4, R2 Score: 0.632697\n",
      "Step: 0, Fold: 5, R2 Score: 0.610445\n",
      "Step: 0, Fold: 6, R2 Score: 0.633423\n",
      "Step: 0, Fold: 7, R2 Score: 0.623502\n",
      "Step: 0, Fold: 8, R2 Score: 0.549991\n",
      "Step: 0, Fold: 9, R2 Score: 0.616767\n",
      "Step: 1, Fold: 0, R2 Score: 0.488044\n",
      "Step: 1, Fold: 1, R2 Score: 0.597659\n",
      "Step: 1, Fold: 2, R2 Score: 0.581527\n",
      "Step: 1, Fold: 3, R2 Score: 0.609118\n",
      "Step: 1, Fold: 4, R2 Score: 0.690443\n",
      "Step: 1, Fold: 5, R2 Score: 0.646245\n",
      "Step: 1, Fold: 6, R2 Score: 0.618062\n",
      "Step: 1, Fold: 7, R2 Score: 0.622375\n",
      "Step: 1, Fold: 8, R2 Score: 0.653062\n",
      "Step: 1, Fold: 9, R2 Score: 0.521284\n",
      "Step: 2, Fold: 0, R2 Score: 0.552939\n",
      "Step: 2, Fold: 1, R2 Score: 0.657563\n",
      "Step: 2, Fold: 2, R2 Score: 0.554994\n",
      "Step: 2, Fold: 3, R2 Score: 0.57154\n",
      "Step: 2, Fold: 4, R2 Score: 0.677295\n",
      "Step: 2, Fold: 5, R2 Score: 0.663605\n",
      "Step: 2, Fold: 6, R2 Score: 0.50206\n",
      "Step: 2, Fold: 7, R2 Score: 0.595221\n",
      "Step: 2, Fold: 8, R2 Score: 0.660609\n",
      "Step: 2, Fold: 9, R2 Score: 0.610833\n",
      "Step: 3, Fold: 0, R2 Score: 0.626636\n",
      "Step: 3, Fold: 1, R2 Score: 0.662763\n",
      "Step: 3, Fold: 2, R2 Score: 0.633517\n",
      "Step: 3, Fold: 3, R2 Score: 0.628213\n",
      "Step: 3, Fold: 4, R2 Score: 0.580633\n",
      "Step: 3, Fold: 5, R2 Score: 0.467198\n",
      "Step: 3, Fold: 6, R2 Score: 0.546105\n",
      "Step: 3, Fold: 7, R2 Score: 0.624023\n",
      "Step: 3, Fold: 8, R2 Score: 0.606628\n",
      "Step: 3, Fold: 9, R2 Score: 0.685624\n",
      "Step: 4, Fold: 0, R2 Score: 0.653716\n",
      "Step: 4, Fold: 1, R2 Score: 0.557595\n",
      "Step: 4, Fold: 2, R2 Score: 0.663724\n",
      "Step: 4, Fold: 3, R2 Score: 0.469635\n",
      "Step: 4, Fold: 4, R2 Score: 0.636523\n",
      "Step: 4, Fold: 5, R2 Score: 0.631353\n",
      "Step: 4, Fold: 6, R2 Score: 0.581301\n",
      "Step: 4, Fold: 7, R2 Score: 0.64077\n",
      "Step: 4, Fold: 8, R2 Score: 0.633688\n",
      "Step: 4, Fold: 9, R2 Score: 0.586645\n",
      "Mean R2: 0.604712\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "###################\n",
    "# Load & process data\n",
    "###################\n",
    "def get_data():\n",
    "    #################\n",
    "    # read datasets\n",
    "    #################\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test_submit = pd.read_csv('data/test.csv')\n",
    "\n",
    "    # Get y and ID\n",
    "    train = train[train.y < 250] # Optional: Drop y outliers\n",
    "    y_train = train['y']\n",
    "    train = train.drop('y', 1)\n",
    "    test_submit_id = test_submit['ID']\n",
    "\n",
    "    #########################\n",
    "    # Create data\n",
    "    #########################\n",
    "    features = ['X0',\n",
    "                'X5',\n",
    "                'X118',\n",
    "                'X127',\n",
    "                'X47',\n",
    "                'X315',\n",
    "                'X311',\n",
    "                'X179',\n",
    "                'X314',\n",
    "                'X232',\n",
    "                'X29',\n",
    "                'X263',\n",
    "                'X261']\n",
    "\n",
    "    # Build a new dataset using key parameters, lots of drops\n",
    "    train = train[features]\n",
    "    test_submit = test_submit[features]\n",
    "\n",
    "    # Label encoder\n",
    "    for c in train.columns:\n",
    "        if train[c].dtype == 'object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train[c].values) + list(test_submit[c].values))\n",
    "            train[c] = lbl.transform(list(train[c].values))\n",
    "            test_submit[c] = lbl.transform(list(test_submit[c].values))\n",
    "\n",
    "    # Convert to matrix\n",
    "    train = train.as_matrix()\n",
    "    y_train = np.transpose([y_train.as_matrix()])\n",
    "    test_submit = test_submit.as_matrix()\n",
    "    test_submit_id = test_submit_id.as_matrix()\n",
    "\n",
    "    #print(train.shape)\n",
    "    #print(test_submit.shape)\n",
    "\n",
    "    return train, y_train, test_submit, test_submit_id\n",
    "\n",
    "#####################\n",
    "# Neural Network\n",
    "#####################\n",
    "# Training steps\n",
    "STEPS = 500\n",
    "LEARNING_RATE = 0.0001\n",
    "BETA = 0.01\n",
    "DROPOUT = 0.5\n",
    "RANDOM_SEED = 12345\n",
    "MAX_Y = 250\n",
    "RESTORE = True\n",
    "START = 0\n",
    "\n",
    "# Training variables\n",
    "IN_DIM = 13\n",
    "\n",
    "# Network Parameters - Hidden layers\n",
    "n_hidden_1 = 100\n",
    "n_hidden_2 = 50\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.03, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def deep_network(inputs, keep_prob):\n",
    "    # Input -> Hidden Layer\n",
    "    w1 = weight_variable([IN_DIM, n_hidden_1])\n",
    "    b1 = bias_variable([n_hidden_1])\n",
    "    # Hidden Layer -> Hidden Layer\n",
    "    w2 = weight_variable([n_hidden_1, n_hidden_2])\n",
    "    b2 = bias_variable([n_hidden_2])\n",
    "    # Hidden Layer -> Output\n",
    "    w3 = weight_variable([n_hidden_2, 1])\n",
    "    b3 = bias_variable([1])\n",
    "\n",
    "    # 1st Hidden layer with dropout\n",
    "    h1 = tf.nn.relu(tf.matmul(inputs, w1) + b1)\n",
    "    h1_dropout = tf.nn.dropout(h1, keep_prob)\n",
    "    # 2nd Hidden layer with dropout\n",
    "    h2 = tf.nn.relu(tf.matmul(h1_dropout, w2) + b2)\n",
    "    h2_dropout = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "    # Run sigmoid on output to get 0 to 1\n",
    "    out = tf.nn.sigmoid(tf.matmul(h2_dropout, w3) + b3)\n",
    "\n",
    "    # Loss function with L2 Regularization\n",
    "    regularizers = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3)\n",
    "\n",
    "    scaled_out = tf.multiply(out, MAX_Y)  # Scale output\n",
    "    return inputs, out, scaled_out, regularizers\n",
    "\n",
    "def main(_):\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, IN_DIM])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # Dropout on hidden layers\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "    # Build the graph for the deep net\n",
    "    inputs, out, scaled_out, regularizers = deep_network(x, keep_prob)\n",
    "\n",
    "    # Normal loss function (RMSE)\n",
    "    loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_, scaled_out))))\n",
    "\n",
    "    # Loss function with L2 Regularization\n",
    "    loss = tf.reduce_mean(loss + BETA * regularizers)\n",
    "\n",
    "    # Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    total_error = tf.reduce_sum(tf.square(tf.subtract(y_, tf.reduce_mean(y_))))\n",
    "    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y_, scaled_out)))\n",
    "    accuracy = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "\n",
    "    # Save model\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        #if RESTORE:\n",
    "        #    print('Loading Model...')\n",
    "        #    ckpt = tf.train.get_checkpoint_state('./models/neural/')\n",
    "        #    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        train, y_train, test_submit, test_submit_id = get_data()\n",
    "\n",
    "        # Train until maximum steps reached or interrupted\n",
    "        for i in range(START, STEPS):\n",
    "            k_fold = KFold(n_splits=10, shuffle=True)\n",
    "            #if i % 100 == 0:\n",
    "            #    saver.save(sess, './models/neural/step_' + str(i) + '.cptk')\n",
    "\n",
    "            for k, (ktrain, ktest) in enumerate(k_fold.split(train, y_train)):\n",
    "                train_step.run(feed_dict={x: train[ktrain], y_: y_train[ktrain], keep_prob: DROPOUT})\n",
    "                # Show test score every 10 iterations\n",
    "                if i % 10 == 0:\n",
    "                    # Tensorflow R2\n",
    "                    #train_accuracy = accuracy.eval(feed_dict={\n",
    "                    #    x: train[ktest], y_: y_train[ktest]})\n",
    "                    # SkLearn metrics R2\n",
    "                    train_accuracy = r2_score(y_train[ktest],\n",
    "                                              sess.run(scaled_out, feed_dict={x: train[ktest], keep_prob: 1.0}))\n",
    "                    print('Step: %d, Fold: %d, R2 Score: %g' % (i, k, train_accuracy))\n",
    "\n",
    "        ####################\n",
    "        # CV (repeat 5 times)\n",
    "        ####################\n",
    "        CV = []\n",
    "        for i in range(5):\n",
    "            k_fold = KFold(n_splits=10, shuffle=True)\n",
    "            for k, (ktrain, ktest) in enumerate(k_fold.split(train, y_train)):\n",
    "                # Tensorflow R2\n",
    "                #accuracy = accuracy.eval(feed_dict={\n",
    "                #    x: train[ktest], y_: y_train[ktest]})\n",
    "                # SkLearn metrics R2\n",
    "                accuracy = r2_score(y_train[ktest],\n",
    "                                          sess.run(scaled_out, feed_dict={x: train[ktest], keep_prob: 1.0}))\n",
    "                print('Step: %d, Fold: %d, R2 Score: %g' % (i, k, accuracy))\n",
    "                CV.append(accuracy)\n",
    "        print('Mean R2: %g' % (np.mean(CV)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
